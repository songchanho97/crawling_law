# -*- coding: utf-8 -*-
import re
import json
from typing import List, Dict, Any, Tuple, Optional

# ====================================
# ì„¤ì •
# ====================================
# ë©”ì¸ ì‹¤í–‰ë¶€ì—ì„œ ë™ì í• ë‹¹
LAW_TITLE = ""
LAW_PREFIX = ""

# í•­(â‘ ~â‘³) ë§¤í•‘ ë° ì •ê·œì‹
CIRCLED_CHARS = [
    "â‘ ",
    "â‘¡",
    "â‘¢",
    "â‘£",
    "â‘¤",
    "â‘¥",
    "â‘¦",
    "â‘§",
    "â‘¨",
    "â‘©",
    "â‘ª",
    "â‘«",
    "â‘¬",
    "â‘­",
    "â‘®",
    "â‘¯",
    "â‘°",
    "â‘±",
    "â‘²",
    "â‘³",
]
CIRCLED_MAP = {ch: i + 1 for i, ch in enumerate(CIRCLED_CHARS)}
CIRCLED_RE = re.compile("|".join(map(re.escape, CIRCLED_CHARS)))

# ì¡°(ì œnì¡°/ì œnì¡°ì˜m) íŒ¨í„´: (ì œëª©)ì€ ì„ íƒ
# group(1)=ë³¸ì¡°ë²ˆí˜¸, group(2)=ì˜ë²ˆí˜¸(optional), group(3)=ì œëª©(optional)
JOSA_RE = re.compile(
    r"^ì œ\s*(\d+)(?:\s*ì¡°ì˜\s*(\d+)|\s*ì¡°)(?:\(([^)]*)\))?", re.MULTILINE
)

# í…ìŠ¤íŠ¸í˜• í•­ ë³´ì¡° ì‹ë³„ì: ë¬¸ë‹¨ ì‹œì‘ì—ì„œ 'ì œ n í•­'
HANG_TEXT_RE = re.compile(r"(?m)^\s*ì œ\s*(\d+)\s*í•­\b")

# í˜¸: ë¬¸ë‹¨ ì‹œì‘ '1. ', '2. ' â€¦
HO_LINE_RE = re.compile(r"(?m)^\s*(\d+)\.\s")


# ====================================
# ìœ í‹¸
# ====================================
def normalize_text(s: str) -> str:
    """ì¤„ë°”ê¿ˆ/ìŠ¤í˜ì´ìŠ¤ ì •ê·œí™”"""
    s = s.replace("\r\n", "\n").replace("\r", "\n")
    s = s.replace("\xa0", " ")
    s = re.sub(r"[ \t]+", " ", s)
    return s.strip()


def make_article_id(main_no: str, sub_no: Optional[str] = None) -> str:
    """ì œ4ì¡°â†’ì‚°ì—…ì•ˆì „ë³´ê±´ë²•ì‹œí–‰ê·œì¹™-4, ì œ4ì¡°ì˜2â†’ì‚°ì—…ì•ˆì „ë³´ê±´ë²•ì‹œí–‰ê·œì¹™-4_2"""
    return f"{LAW_PREFIX}-{main_no}_{sub_no}" if sub_no else f"{LAW_PREFIX}-{main_no}"


def make_article_number_field(main_no: str, sub_no: Optional[str] = None) -> str:
    """number í•„ë“œ: 4, 4ì˜2"""
    return f"{main_no}ì˜{sub_no}" if sub_no else str(main_no)


def split_by_articles(full_text: str) -> List[Tuple[str, Optional[str], str, int, int]]:
    """
    ì „ì²´ ë¬¸ì„œë¥¼ ì¡° ë‹¨ìœ„ ë¸”ë¡ìœ¼ë¡œ ë¶„í• .
    ë°˜í™˜: [(ë³¸ì¡°ë²ˆí˜¸, ì˜ë²ˆí˜¸ or None, ì œëª©, start, end)]
    """
    matches = list(JOSA_RE.finditer(full_text))
    chunks: List[Tuple[str, Optional[str], str, int, int]] = []
    for i, m in enumerate(matches):
        main_no = m.group(1)
        sub_no = m.group(2)  # None or '2'
        title = m.group(3) or ""
        start = m.start()
        end = matches[i + 1].start() if i + 1 < len(matches) else len(full_text)
        chunks.append((main_no, sub_no, title, start, end))
    return chunks


def find_first_hang_start(block: str) -> int:
    """
    ë¸”ë¡ ë‚´ 'í•­' ì‹œì‘ ìœ„ì¹˜(â‘  ë˜ëŠ” ë¬¸ë‹¨ ì‹œì‘ 'ì œní•­')ì˜ ì¸ë±ìŠ¤ ë°˜í™˜. ì—†ìœ¼ë©´ -1
    """
    p1 = CIRCLED_RE.search(block)
    pos1 = p1.start() if p1 else -1
    p2 = HANG_TEXT_RE.search(block)
    pos2 = p2.start() if p2 else -1
    if pos1 == -1 and pos2 == -1:
        return -1
    if pos1 == -1:
        return pos2
    if pos2 == -1:
        return pos1
    return min(pos1, pos2)


def find_hang_positions(block_text: str) -> List[Tuple[int, int, str]]:
    """
    â‘ â‘¡â€¦ í•­ ìœ„ì¹˜ ëª©ë¡: [(index, ë²ˆí˜¸, ê¸°í˜¸)]
    (í…ìŠ¤íŠ¸í˜• 'ì œní•­'ì€ ë¶„í•  ê¸°ì¤€ì—ì„œ ì œì™¸. ì‹¤ì œ íŒŒì¼ì€ â‘  í˜•íƒœê°€ ì£¼ë¥˜)
    """
    positions: List[Tuple[int, int, str]] = []
    for m in CIRCLED_RE.finditer(block_text):
        sym = m.group(0)
        positions.append((m.start(), CIRCLED_MAP[sym], sym))
    positions.sort(key=lambda x: x[0])
    return positions


def split_hang_texts(
    block_text: str, hang_positions: List[Tuple[int, int, str]]
) -> List[Tuple[int, str]]:
    """
    í•­ ë¶„ë¦¬: [(í•­ë²ˆí˜¸, í•­ í…ìŠ¤íŠ¸)]. ì„ ë‘ â‘  ê¸°í˜¸ ì œê±°.
    """
    parts: List[Tuple[int, str]] = []
    if not hang_positions:
        return parts
    for i, (pos, num, sym) in enumerate(hang_positions):
        start = pos
        end = (
            hang_positions[i + 1][0] if i + 1 < len(hang_positions) else len(block_text)
        )
        raw = block_text[start:end].lstrip()
        if raw.startswith(sym):
            raw = raw[len(sym) :].lstrip()
        parts.append((num, raw.rstrip()))
    return parts


def split_ho_with_preface(hang_text: str) -> Tuple[str, List[Tuple[int, str]]]:
    """
    í•­ í…ìŠ¤íŠ¸ì—ì„œ 'í˜¸'ë¥¼ ë¶„ë¦¬í•˜ë˜,
    - ë°˜í™˜1: í•­ ë¨¸ë¦¬ë§(ì²« '1.' ì´ì „ í…ìŠ¤íŠ¸, ì—†ìœ¼ë©´ ì „ì²´)
    - ë°˜í™˜2: [(í˜¸ë²ˆí˜¸, í˜¸ í…ìŠ¤íŠ¸)] ëª©ë¡
    """
    matches = list(HO_LINE_RE.finditer(hang_text))
    if not matches:
        # í˜¸ê°€ ì—†ìœ¼ë©´ í•­ í…ìŠ¤íŠ¸ ê·¸ëŒ€ë¡œ ë¨¸ë¦¬ë§ë¡œ ë°˜í™˜
        return hang_text.strip(), []

    preface_end = matches[0].start()
    preface = hang_text[:preface_end].strip()

    results: List[Tuple[int, str]] = []
    for i, m in enumerate(matches):
        ho_no = int(m.group(1))
        start = m.start()
        end = matches[i + 1].start() if i + 1 < len(matches) else len(hang_text)
        piece = hang_text[start:end].strip()
        piece = re.sub(r"^\s*\d+\.\s*", "", piece)  # '1. ' ì œê±°
        results.append((ho_no, piece.strip()))
    return preface, results


def hard_cut_article_text(article_text: str) -> str:
    """
    ì¡°.textì— â‘  ë˜ëŠ” ë¬¸ë‹¨ ì‹œì‘ 'ì œní•­'ì´ ë“¤ì–´ìˆìœ¼ë©´ ê°•ì œ ì»·(ì´ì¤‘ ì•ˆì „ì¥ì¹˜)
    """
    m = CIRCLED_RE.search(article_text)
    if m:
        return article_text[: m.start()].rstrip()
    m2 = HANG_TEXT_RE.search(article_text)
    if m2:
        return article_text[: m2.start()].rstrip()
    return article_text.rstrip()


# ====================================
# ë©”ì¸ ë¹Œë”
# ====================================
def build_nodes(full_text: str) -> List[Dict[str, Any]]:
    """
    ê·œì¹™:
      - ì¡°.text: 'ì œnì¡°(ì œëª©)' + (ìˆë‹¤ë©´ â‘  ì´ì „ í”„ë¡¤ë¡œê·¸ê¹Œì§€ë§Œ)
      - í•­.text: 'í˜¸' ëª©ë¡ ì•ì˜ ë¨¸ë¦¬ë§ë§Œ
      - í˜¸.text: ê° '1. â€¦' í•­ëª© ë³¸ë¬¸
      - í•­(â‘  ë“±)ì´ ì „í˜€ ì—†ìœ¼ë©´ ì¡°ë§Œ ìƒì„±(ì •ì˜í˜• ì¡° ë“±)
      - refsëŠ” í•­ìƒ []
    """
    nodes: List[Dict[str, Any]] = []
    node_map: Dict[str, Dict[str, Any]] = {}

    for main_no, sub_no, title, start, end in split_by_articles(full_text):
        block = full_text[start:end].strip()

        # ì¡° í—¤ë”(ì œnì¡°(ì œëª©)) ë¬¸ìì—´
        m_head = JOSA_RE.match(block)
        header_txt = (
            m_head.group(0).strip() if m_head else block.split("\n", 1)[0].strip()
        )

        # ë¸”ë¡ ë‚´ ì²« 'í•­' ì‹œì‘ ìœ„ì¹˜(â‘  ë˜ëŠ” ë¬¸ë‹¨ ì‹œì‘ 'ì œní•­')
        first_hang_idx = find_first_hang_start(block)

        # ì¡° í…ìŠ¤íŠ¸(í—¤ë” + â‘  ì´ì „ í”„ë¡¤ë¡œê·¸)
        if first_hang_idx != -1 and m_head:
            preface = block[m_head.end() : first_hang_idx].strip()
            article_text = header_txt if not preface else (header_txt + "\n" + preface)
        else:
            # í•­ ìì²´ê°€ ì—†ìœ¼ë©´ ì „ì²´ ë¸”ë¡(ìš”ì²­ì‚¬í•­)
            article_text = block

        # ì´ì¤‘ ì•ˆì „ì¥ì¹˜: ì¡°.textì—ì„œ â‘ /ì œní•­ ë“±ì¥ ì‹œ ë¬´ì¡°ê±´ ì»·
        article_text = hard_cut_article_text(article_text)

        # ì¡° ë…¸ë“œ ìƒì„±
        article_id = make_article_id(main_no, sub_no)
        number_field = make_article_number_field(main_no, sub_no)
        art_node = {
            "id": article_id,
            "law_title": LAW_TITLE,
            "level": "ì¡°",
            "number": number_field,
            "parent_id": None,
            "Children_id": [],
            "text": article_text,
            "refs": [],
        }
        nodes.append(art_node)
        node_map[article_id] = art_node

        # í•­ ë¶„í•´: â‘  ì—†ìœ¼ë©´ ì¢…ë£Œ
        if first_hang_idx == -1:
            continue

        # â‘  ì´í›„ë§Œ ì˜ë¼ì„œ í•­ ë¶„í•´
        after_header = block[first_hang_idx:]
        hang_positions = find_hang_positions(after_header)
        hang_parts = split_hang_texts(after_header, hang_positions)

        for hang_no, hang_txt in hang_parts:
            # í•­ì˜ ë¨¸ë¦¬ë§/í˜¸ ë¶„ë¦¬ (í•µì‹¬ ìˆ˜ì •)
            hang_preface, ho_list = split_ho_with_preface(hang_txt)

            hang_id = f"{article_id}({hang_no})"
            hang_node = {
                "id": hang_id,
                "law_title": LAW_TITLE,
                "level": "í•­",
                "number": str(hang_no),
                "parent_id": article_id,
                "Children_id": [],
                "text": hang_preface,  # âœ… í•­.textì—ëŠ” ë¨¸ë¦¬ë§ë§Œ
                "refs": [],
            }
            nodes.append(hang_node)
            node_map[article_id]["Children_id"].append(hang_id)
            node_map[hang_id] = hang_node

            # í˜¸ ë¶„í•´
            for ho_no, ho_txt in ho_list:
                ho_id = f"{hang_id}[{ho_no}]"
                ho_node = {
                    "id": ho_id,
                    "law_title": LAW_TITLE,
                    "level": "í˜¸",
                    "number": str(ho_no),
                    "parent_id": hang_id,
                    "Children_id": [],
                    "text": ho_txt,
                    "refs": [],
                }
                nodes.append(ho_node)
                node_map[hang_id]["Children_id"].append(ho_id)
                node_map[ho_id] = ho_node

    return nodes


# ====================================
# ì‹¤í–‰ (ê²½ë¡œë§Œ ë°”ê¿”ì„œ ì‚¬ìš©)
# ====================================
# ìˆ˜ì •2
if __name__ == "__main__":
    # ğŸ”½ ìˆ˜ì • 1: ì²˜ë¦¬í•  íŒŒì¼ ì´ë¦„ ëª©ë¡ì„ ì—¬ê¸°ì— ì¶”ê°€í•©ë‹ˆë‹¤. (í™•ì¥ì ì œì™¸)
    file_names_to_process = [
        "í•´ì²´ê³µì‚¬í‘œì¤€ì•ˆì „ì‘ì—…ì§€ì¹¨",
        "ì¶”ë½ì¬í•´ë°©ì§€í‘œì¤€ì•ˆì „ì‘ì—…ì§€ì¹¨",
        "ìœ í•´Â·ìœ„í—˜ë°©ì§€ê³„íšì„œ ìì²´ì‹¬ì‚¬ ë° í™•ì¸ì—…ì²´ ì§€ì •ëŒ€ìƒ ê±´ì„¤ì—…ì²´ ê³ ì‹œ",  # í•´ë‹¹ ë°ì´í„°ëŠ” ê°€ì ¸ì˜¬ ìˆ˜ ì—†ìŒ
        "ë³´í˜¸êµ¬ ììœ¨ì•ˆì „í™•ì¸ ê³ ì‹œ",
        "ê±´ì„¤ì—… ìœ í•´Â·ìœ„í—˜ë°©ì§€ê³„íšì„œ ì¤‘ ì§€ë„ì‚¬ê°€ í‰ê°€Â·í™•ì¸ í•  ìˆ˜ ìˆëŠ” ëŒ€ìƒ ê±´ì„¤ê³µì‚¬ì˜ ë²”ìœ„ ë° ì§€ë„ì‚¬ì˜ ìš”ê±´",
        "ê°€ì„¤ê³µì‚¬ í‘œì¤€ì•ˆì „ ì‘ì—…ì§€ì¹¨",
        "ë°©í˜¸ì¥ì¹˜ ì•ˆì „ì¸ì¦ ê³ ì‹œ",
        "ì•ˆì „ì¸ì¦Â·ììœ¨ì•ˆì „í™•ì¸ì‹ ê³ ì˜ ì ˆì°¨ì— ê´€í•œ ê³ ì‹œ",
        "ë°©í˜¸ì¥ì¹˜ ììœ¨ì•ˆì „ê¸°ì¤€ ê³ ì‹œ",
        "êµ´ì°©ê³µì‚¬ í‘œì¤€ì•ˆì „ ì‘ì—…ì§€ì¹¨",
        "ìœ„í—˜ê¸°ê³„Â·ê¸°êµ¬ ì•ˆì „ì¸ì¦ ê³ ì‹œ",
        "ê±´ì„¤ì—…ì²´ì˜ ì‚°ì—…ì¬í•´ì˜ˆë°©í™œë™ ì‹¤ì  í‰ê°€ê¸°ì¤€",
        "ì•ˆì „ë³´ê±´êµìœ¡ê·œì •",
        "ê±´ì„¤ê³µì‚¬ ì•ˆì „ë³´ê±´ëŒ€ì¥ì˜ ì‘ì„± ë“±ì— ê´€í•œ ê³ ì‹œ",
        "ê±´ì„¤ì—… ì‚°ì—…ì•ˆì „ë³´ê±´ê´€ë¦¬ë¹„ ê³„ìƒ ë° ì‚¬ìš©ê¸°ì¤€",
        "ì‚°ì—…ì¬í•´ì˜ˆë°©ì‹œì„¤ìê¸ˆ ìœµìê¸ˆ ì§€ì›ì‚¬ì—… ë° í´ë¦°ì‚¬ì—…ì¥ ì¡°ì„±ì§€ì›ì‚¬ì—… ìš´ì˜ê·œì •",
    ]

    # ğŸ”½ ìˆ˜ì • 2: ë°˜ë³µë¬¸ìœ¼ë¡œ ê° íŒŒì¼ì„ ìˆœì°¨ì ìœ¼ë¡œ ì²˜ë¦¬í•©ë‹ˆë‹¤.
    for file_name in file_names_to_process:
        print(f"\nâ–¶ï¸ '{file_name}' íŒŒì¼ ì²˜ë¦¬ ì‹œì‘...")

        # ì „ì—­ ë³€ìˆ˜ì¸ LAW_TITLEê³¼ LAW_PREFIXë¥¼ í˜„ì¬ íŒŒì¼ëª…ìœ¼ë¡œ ë™ì  ì—…ë°ì´íŠ¸
        LAW_TITLE = file_name
        LAW_PREFIX = file_name

        input_path = f"./data/ê³ ì‹œë°ì˜ˆê·œ/{file_name}_ì›ë¬¸.txt"
        output_path = f"./data/ê³ ì‹œë°ì˜ˆê·œ/{file_name}_í°í‹€.json"

        try:
            with open(input_path, "r", encoding="utf-8") as f:
                raw = f.read()

            text = normalize_text(raw)
            nodes = build_nodes(text)

            # ê²€ì¦: í•­.textì— '1.'ì´ ë‚¨ì•„ìˆìœ¼ë©´ ê²½ê³ 
            bad_hang = [
                n["id"]
                for n in nodes
                if n["level"] == "í•­" and HO_LINE_RE.search(n["text"])
            ]
            if bad_hang:
                print(
                    f"[ê²½ê³ ] '{file_name}' ì²˜ë¦¬ ì¤‘, ì¼ë¶€ í•­.textì— 'í˜¸'ê°€ ë‚¨ì•„ìˆì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤. (ë…¸ë“œ: {bad_hang[:3]})"
                )

            with open(output_path, "w", encoding="utf-8") as f:
                json.dump(nodes, f, ensure_ascii=False, indent=2)

            print(f"âœ… [ì™„ë£Œ] {len(nodes)}ê°œ ë…¸ë“œ ì €ì¥ â†’ {output_path}")

        except FileNotFoundError:
            print(f"âŒ [ì˜¤ë¥˜] ì…ë ¥ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {input_path}")
        except Exception as e:
            print(f"âŒ [ì˜¤ë¥˜] '{file_name}' ì²˜ë¦¬ ì¤‘ ì˜ˆì™¸ ë°œìƒ: {e}")

    print("\nğŸ‰ ëª¨ë“  íŒŒì¼ ì²˜ë¦¬ê°€ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.")
