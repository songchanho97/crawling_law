import pandas as pd
import re
from selenium import webdriver
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from webdriver_manager.chrome import ChromeDriverManager
import time


def get_sfon_number(element):
    """ë§í¬ ìš”ì†Œì˜ í´ë˜ìŠ¤ì—ì„œ sfon ë²ˆí˜¸ë¥¼ ì •ìˆ˜ë¡œ ì¶”ì¶œí•©ë‹ˆë‹¤. ì—†ìœ¼ë©´ 0ì„ ë°˜í™˜í•©ë‹ˆë‹¤."""
    class_attr = element.get_attribute("class") or ""
    match = re.search(r"sfon(\d+)", class_attr)
    return int(match.group(1)) if match else 0


def scrape_law_data_with_clicks(url, output_filename):
    """
    'ì‹œí–‰ë ¹', 'ì‹œí–‰ê·œì¹™' í˜ì´ì§€ìš© í¬ë¡¤ë§ í•¨ìˆ˜ (í…Œì´ë¸” ì¶”ì¶œ ê¸°ëŠ¥ ì¶”ê°€)
    """
    driver = None
    print("-" * 50)
    print(f"â–¶ï¸ ì‘ì—…ì„ ì‹œì‘í•©ë‹ˆë‹¤ (ìœ í˜• 1 - ëª©ë¡í˜•): {output_filename}")
    try:
        service = Service(ChromeDriverManager().install())
        options = webdriver.ChromeOptions()
        options.add_argument("--headless")
        options.add_argument("--window-size=1920x1080")
        options.add_argument("--log-level=3")
        driver = webdriver.Chrome(service=service, options=options)
        driver.maximize_window()
        driver.get(url)
        wait = WebDriverWait(driver, 20)

        try:
            wait.until(EC.presence_of_element_located((By.CSS_SELECTOR, "div.lawcon")))
            law_articles = driver.find_elements(By.CSS_SELECTOR, "div.lawcon")
        except:
            # 'div.lawcon'ì´ ì—†ëŠ” í˜ì´ì§€ëŠ” ë§í¬ê°€ ì—†ëŠ” ë‹¨ìˆœ ê³ ì‹œì¼ ìˆ˜ ìˆìœ¼ë¯€ë¡œ, ë‹¤ë¥¸ ë°©ë²•ìœ¼ë¡œ í…ìŠ¤íŠ¸ë¥¼ ìˆ˜ì§‘
            try:
                content_div = driver.find_element(By.ID, "content")
                print("âœ… 'div.lawcon'ì´ ì—†ì–´ ë³¸ë¬¸ ì „ì²´ë¥¼ ìˆ˜ì§‘í•©ë‹ˆë‹¤.")
                # ì´ ê²½ìš°, ë§í¬ ë¶„ì„ì´ ë¬´ì˜ë¯¸í•˜ë¯€ë¡œ ë°ì´í„°ë¥¼ ìƒì„±í•˜ê³  í•¨ìˆ˜ë¥¼ ì¢…ë£Œí•©ë‹ˆë‹¤.
                df = pd.DataFrame(
                    [
                        {
                            "ì¡°": "ì „ì²´",
                            "ë§í¬ í…ìŠ¤íŠ¸": "ë³¸ë¬¸ ë‚´ìš©",
                            "ë§í¬í…ìŠ¤íŠ¸ í´ë¦­ì‹œ ë°ì´í„°": content_div.text.strip(),
                        }
                    ]
                )
                df.to_csv(output_filename, index=False, encoding="utf-8-sig")
                print(f"âœ… ì‘ì—… ì™„ë£Œ! '{output_filename}' íŒŒì¼ë¡œ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
                return
            except Exception as e:
                print(
                    f"âŒ ì˜¤ë¥˜: '{url}' í˜ì´ì§€ì—ì„œ 'div.lawcon' ë˜ëŠ” 'content' ìš”ì†Œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. í˜ì´ì§€ êµ¬ì¡°ê°€ ë‹¤ë¥¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ({e})"
                )
                return

        total_articles = len(law_articles)
        final_data_list = []
        print(f"âœ… ì´ {total_articles}ê°œì˜ 'ì¡°'ë¥¼ ë°œê²¬í–ˆìŠµë‹ˆë‹¤. ë¶„ì„ì„ ì‹œì‘í•©ë‹ˆë‹¤.")
        if total_articles > 0:
            print(
                "âš ï¸ ì´ ì‘ì—…ì€ ëª¨ë“  ë§í¬ë¥¼ í´ë¦­í•˜ë¯€ë¡œ ì‹œê°„ì´ ë§¤ìš° ì˜¤ë˜ ê±¸ë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤."
            )

        for i, article_div in enumerate(law_articles):
            article_num = ""
            try:
                title_element = article_div.find_element(By.CSS_SELECTOR, "p.pty1_p4")
                match = re.search(r"ì œ(\d+(?:ì˜\d+)?)ì¡°", title_element.text)
                article_num = match.group(1) if match else "ë²ˆí˜¸ ì—†ìŒ"
            except:
                continue

            if not article_num:
                continue

            p_tags_in_article = article_div.find_elements(By.TAG_NAME, "p")
            for p_tag in p_tags_in_article:
                links = p_tag.find_elements(By.CSS_SELECTOR, 'a.link, a[class*="sfon"]')
                if not links:
                    continue

                link_groups = []
                current_group = []
                for link in links:
                    if not current_group:
                        current_group = [link]
                        continue

                    prev_link = current_group[-1]
                    prev_sfon_num = get_sfon_number(prev_link)
                    current_sfon_num = get_sfon_number(link)
                    prev_class = prev_link.get_attribute("class") or ""
                    current_class = link.get_attribute("class") or ""

                    prev_text = prev_link.text.strip()
                    # =================== âœ¨ ì˜¤íƒ€ ìˆ˜ì •ëœ ë¶€ë¶„ âœ¨ ===================
                    current_text = link.text.strip()  # 'current_link'ë¥¼ 'link'ë¡œ ìˆ˜ì •
                    # ==========================================================

                    is_prev_link_a_law_ref = prev_text.endswith("ã€") or prev_text in [
                        "ê°™ì€ ë²•",
                        "ê°™ì€ ë²• ì‹œí–‰ë ¹",
                        "ê°™ì€ ë²• ì‹œí–‰ê·œì¹™",
                    ]

                    is_current_link_an_article = current_text.startswith("ì œ")

                    is_special_merge_case = (
                        is_prev_link_a_law_ref
                        and is_current_link_an_article
                        and "link" in prev_class
                        and "link" in current_class
                    )

                    break_group = False
                    if not is_special_merge_case:
                        if (
                            prev_sfon_num == 0
                            or current_sfon_num == 0
                            or current_sfon_num <= prev_sfon_num
                        ):
                            break_group = True
                        elif "sfon6" in current_class and "sfon6" not in prev_class:
                            break_group = True

                    if break_group:
                        link_groups.append(current_group)
                        current_group = [link]
                    else:
                        current_group.append(link)

                if current_group:
                    link_groups.append(current_group)

                for group in link_groups:
                    merged_text = " ".join([link.text.strip() for link in group])
                    merged_text = re.sub(r"ã€ ì œ", "ã€ì œ", merged_text)

                    element_to_click = group[-1]
                    new_window_text = ""
                    original_window = driver.current_window_handle
                    try:
                        driver.execute_script("arguments[0].click();", element_to_click)
                        wait.until(EC.number_of_windows_to_be(2))
                        for handle in driver.window_handles:
                            if handle != original_window:
                                driver.switch_to.window(handle)
                                break

                        try:
                            new_text_element = wait.until(
                                EC.presence_of_element_located(
                                    (By.CSS_SELECTOR, "#linkedJoContent")
                                )
                            )
                            new_window_text = new_text_element.text.strip()
                        except Exception:
                            try:
                                wait.until(
                                    EC.presence_of_element_located(
                                        (By.ID, "lsLinkTable")
                                    )
                                )
                                thead = driver.find_element(By.ID, "lsLinkTableTop")
                                tbody = driver.find_element(By.ID, "lsLinkTable")
                                new_window_text = (
                                    f"{thead.text.strip()}\n{tbody.text.strip()}"
                                )
                            except Exception:
                                try:
                                    wait.until(
                                        EC.presence_of_element_located(
                                            (By.CSS_SELECTOR, "select#bylList")
                                        )
                                    )
                                    new_window_text = (
                                        "ë³„í‘œ/ì„œì‹ ë°ì´í„° (ì²˜ë¦¬ ë¡œì§ ìƒëµ)"
                                    )
                                except Exception:
                                    new_window_text = "ì˜¤ë¥˜: ìƒˆ ì°½ì—ì„œ ì•Œë ¤ì§„ ë°ì´í„° í˜•ì‹(#linkedJoContent, Table, #bylList)ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."

                        TEXT_LENGTH_LIMIT = 10000
                        if len(new_window_text) > TEXT_LENGTH_LIMIT:
                            new_window_text = "ë‚´ìš©ì´ ë„ˆë¬´ ê¸¸ì–´ ìˆ˜ì§‘ ì œì™¸"
                    except Exception as e:
                        new_window_text = f"ì˜¤ë¥˜ ë°œìƒ ë˜ëŠ” í…ìŠ¤íŠ¸ ìˆ˜ì§‘ ì‹¤íŒ¨: {e}"
                    finally:
                        while len(driver.window_handles) > 1:
                            for handle in driver.window_handles:
                                if handle != original_window:
                                    driver.switch_to.window(handle)
                                    driver.close()
                                    break
                            time.sleep(0.1)
                        driver.switch_to.window(original_window)

                    final_data_list.append(
                        {
                            "ì¡°": article_num,
                            "ë§í¬ í…ìŠ¤íŠ¸": merged_text,
                            "ë§í¬í…ìŠ¤íŠ¸ í´ë¦­ì‹œ ë°ì´í„°": new_window_text,
                        }
                    )
            if total_articles > 0:
                percentage = (i + 1) / total_articles * 100
                print(
                    f"â³ [ì§„í–‰ë¥ : {percentage:.1f}%] ì œ{article_num}ì¡° ë¶„ì„ ì™„ë£Œ ({i + 1}/{total_articles})"
                )

        if final_data_list:
            df = pd.DataFrame(final_data_list)
            df.to_csv(output_filename, index=False, encoding="utf-8-sig")
            print(f"âœ… ì‘ì—… ì™„ë£Œ! '{output_filename}' íŒŒì¼ë¡œ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
        elif total_articles == 0:
            print(
                f"âš ï¸ '{output_filename}' ì—ì„œ 'ì¡°' ë‹¨ìœ„ ë°ì´í„°ë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. (ì‘ì—…ì€ ì •ìƒ ì¢…ë£Œ)"
            )
        else:
            print("âš ï¸ ìˆ˜ì§‘ëœ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")

    except Exception as e:
        print(f"âŒ '{output_filename}' ì‘ì—… ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}")
    finally:
        if driver:
            driver.quit()


# --- ë©”ì¸ ì‹¤í–‰ë¶€ ---
if __name__ == "__main__":
    list_type_jobs = {
        "ê±´ì„¤ì—… ìœ í•´Â·ìœ„í—˜ë°©ì§€ê³„íšì„œ ì¤‘ ì§€ë„ì‚¬ê°€ í‰ê°€Â·í™•ì¸ í•  ìˆ˜ ìˆëŠ” ëŒ€ìƒ ê±´ì„¤ê³µì‚¬ì˜ ë²”ìœ„ ë° ì§€ë„ì‚¬ì˜ ìš”ê±´": "https://www.law.go.kr/LSW/admRulInfoP.do?admRulSeq=2100000186089&chrClsCd=010202&urlMode=admRulLsInfoP"
        # (ì´í•˜ URL ëª©ë¡ì€ ë™ì¼í•˜ë¯€ë¡œ ìƒëµ)
    }

    for law_name, law_url in list_type_jobs.items():
        output_csv_name = f"./data/{law_name}_data.csv"
        scrape_law_data_with_clicks(law_url, output_csv_name)

    print("\nğŸ‰ ëª¨ë“  ì‘ì—…ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.")
