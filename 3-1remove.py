# -*- coding: utf-8 -*-
import re
import json
import os
import pandas as pd
from typing import List, Dict, Any, Tuple, Optional

# ====================================
# ì²˜ë¦¬í•  íŒŒì¼ ëª©ë¡
# ====================================
# ì—¬ê¸°ì— ì²˜ë¦¬í•  íŒŒì¼ì˜ ê¸°ë³¸ ê²½ë¡œë¥¼ ì¶”ê°€í•˜ì„¸ìš”.
# ì˜ˆ: "./data/ì‚°ì—…ì•ˆì „ë³´ê±´ë²•_ì‹œí–‰ë ¹"
# ì…ë ¥ íŒŒì¼: {ê¸°ë³¸ê²½ë¡œ}_labeled.xlsx
# ì¶œë ¥ íŒŒì¼: {ê¸°ë³¸ê²½ë¡œ}_Ref_labeled_with_json.xlsx
# --------------------------------------------------------------------------
FILES_TO_PROCESS = [
    "./data/ê³ ì‹œë°ì˜ˆê·œ/ì¤‘ëŒ€ì¬í•´ì²˜ë²Œë²•_ì‹œí–‰ë ¹",
    "./data/ê³ ì‹œë°ì˜ˆê·œ/ì‚°ì—…ì•ˆì „ë³´ê±´ë²•",
    "./data/ê³ ì‹œë°ì˜ˆê·œ/í™”í•™ë¬¼ì§ˆê´€ë¦¬ë²•",
    "./data/ê³ ì‹œë°ì˜ˆê·œ/í•´ì²´ê³µì‚¬í‘œì¤€ì•ˆì „ì‘ì—…ì§€ì¹¨",
    "./data/ê³ ì‹œë°ì˜ˆê·œ/ì¶”ë½ì¬í•´ë°©ì§€í‘œì¤€ì•ˆì „ì‘ì—…ì§€ì¹¨",
    "./data/ê³ ì‹œë°ì˜ˆê·œ/ìœ í•´Â·ìœ„í—˜ë°©ì§€ê³„íšì„œ ìì²´ì‹¬ì‚¬ ë° í™•ì¸ì—…ì²´ ì§€ì •ëŒ€ìƒ ê±´ì„¤ì—…ì²´ ê³ ì‹œ",
    "./data/ê³ ì‹œë°ì˜ˆê·œ/ë³´í˜¸êµ¬ ììœ¨ì•ˆì „í™•ì¸ ê³ ì‹œ",
    "./data/ê³ ì‹œë°ì˜ˆê·œ/ê±´ì„¤ì—… ìœ í•´Â·ìœ„í—˜ë°©ì§€ê³„íšì„œ ì¤‘ ì§€ë„ì‚¬ê°€ í‰ê°€Â·í™•ì¸ í•  ìˆ˜ ìˆëŠ” ëŒ€ìƒ ê±´ì„¤ê³µì‚¬ì˜ ë²”ìœ„ ë° ì§€ë„ì‚¬ì˜ ìš”ê±´",
    "./data/ê³ ì‹œë°ì˜ˆê·œ/ê°€ì„¤ê³µì‚¬ í‘œì¤€ì•ˆì „ ì‘ì—…ì§€ì¹¨",
    "./data/ê³ ì‹œë°ì˜ˆê·œ/ë°©í˜¸ì¥ì¹˜ ì•ˆì „ì¸ì¦ ê³ ì‹œ",
    # "./data/ì•ˆì „ì¸ì¦Â·ììœ¨ì•ˆì „í™•ì¸ì‹ ê³ ì˜ ì ˆì°¨ì— ê´€í•œ ê³ ì‹œ",
    # "./data/ë°©í˜¸ì¥ì¹˜ ììœ¨ì•ˆì „ê¸°ì¤€ ê³ ì‹œ",
    # "./data/êµ´ì°©ê³µì‚¬ í‘œì¤€ì•ˆì „ ì‘ì—…ì§€ì¹¨",
    # "./data/ìœ„í—˜ê¸°ê³„Â·ê¸°êµ¬ ì•ˆì „ì¸ì¦ ê³ ì‹œ",
    # "./data/ê±´ì„¤ì—…ì²´ì˜ ì‚°ì—…ì¬í•´ì˜ˆë°©í™œë™ ì‹¤ì  í‰ê°€ê¸°ì¤€",
    # "./data/ì•ˆì „ë³´ê±´êµìœ¡ê·œì •",
    # "./data/ê±´ì„¤ê³µì‚¬ ì•ˆì „ë³´ê±´ëŒ€ì¥ì˜ ì‘ì„± ë“±ì— ê´€í•œ ê³ ì‹œ",
    # "./data/ê±´ì„¤ì—… ì‚°ì—…ì•ˆì „ë³´ê±´ê´€ë¦¬ë¹„ ê³„ìƒ ë° ì‚¬ìš©ê¸°ì¤€",
    # "./data/ì‚°ì—…ì¬í•´ì˜ˆë°©ì‹œì„¤ìê¸ˆ ìœµìê¸ˆ ì§€ì›ì‚¬ì—… ë° í´ë¦°ì‚¬ì—…ì¥ ì¡°ì„±ì§€ì›ì‚¬ì—… ìš´ì˜ê·œì •",
]


# ====================================
# ì •ê·œì‹ (ê¸°ì¡´ê³¼ ë™ì¼)
# ====================================
CIRCLED_CHARS = [
    "â‘ ",
    "â‘¡",
    "â‘¢",
    "â‘£",
    "â‘¤",
    "â‘¥",
    "â‘¦",
    "â‘§",
    "â‘¨",
    "â‘©",
    "â‘ª",
    "â‘«",
    "â‘¬",
    "â‘­",
    "â‘®",
    "â‘¯",
    "â‘°",
    "â‘±",
    "â‘²",
    "â‘³",
]
CIRCLED_MAP = {ch: i + 1 for i, ch in enumerate(CIRCLED_CHARS)}
CIRCLED_RE = re.compile("|".join(map(re.escape, CIRCLED_CHARS)))
JOSA_RE = re.compile(
    r"^ì œ\s*(\d+)(?:\s*ì¡°ì˜\s*(\d+)|\s*ì¡°)(?:\(([^)]*)\))?", re.MULTILINE
)
HANG_TEXT_RE = re.compile(r"(?m)^\s*ì œ\s*(\d+)\s*í•­\b")
HO_LINE_RE = re.compile(r"(?m)^\s*(\d+)\.\s")
META_LINE_RE = re.compile(r"^(?:\s*\[[^\]]+\]\s*)+$")


# ====================================
# ìœ í‹¸ë¦¬í‹° ë° íŒŒì‹± í•¨ìˆ˜ (ê¸°ì¡´ê³¼ ë™ì¼)
# ====================================
def normalize_text(s: str) -> str:
    s = s.replace("\r\n", "\n").replace("\r", "\n")
    s = s.replace("\xa0", " ")
    s = re.sub(r"[ \t]+", " ", s)
    return s.strip()


def extract_law_title_and_body(raw: str) -> Tuple[str, str]:
    if not isinstance(raw, str):
        return "", ""
    txt = normalize_text(raw)
    lines = [ln for ln in txt.split("\n")]
    title = ""
    i = 0
    while i < len(lines) and not title:
        cand = lines[i].strip()
        if cand:
            title = cand
        i += 1
    if not title:
        return "", ""
    while i < len(lines) and META_LINE_RE.match(lines[i].strip() or ""):
        i += 1
    body = "\n".join(lines[i:]).strip()
    return title, body


def make_article_id(prefix: str, main_no: str, sub_no: Optional[str] = None) -> str:
    return f"{prefix}-{main_no}_{sub_no}" if sub_no else f"{prefix}-{main_no}"


def make_article_number_field(main_no: str, sub_no: Optional[str] = None) -> str:
    return f"{main_no}ì˜{sub_no}" if sub_no else str(main_no)


def split_by_articles(full_text: str) -> List[Tuple[str, Optional[str], str, int, int]]:
    matches = list(JOSA_RE.finditer(full_text))
    chunks: List[Tuple[str, Optional[str], str, int, int]] = []
    for i, m in enumerate(matches):
        main_no = m.group(1)
        sub_no = m.group(2)
        title = m.group(3) or ""
        start = m.start()
        end = matches[i + 1].start() if i + 1 < len(matches) else len(full_text)
        chunks.append((main_no, sub_no, title, start, end))
    return chunks


def find_first_hang_start(block: str) -> int:
    p1 = CIRCLED_RE.search(block)
    pos1 = p1.start() if p1 else -1
    p2 = HANG_TEXT_RE.search(block)
    pos2 = p2.start() if p2 else -1
    if pos1 == -1 and pos2 == -1:
        return -1
    if pos1 == -1:
        return pos2
    if pos2 == -1:
        return pos1
    return min(pos1, pos2)


def find_hang_positions(block_text: str):
    positions = []
    for m in CIRCLED_RE.finditer(block_text):
        sym = m.group(0)
        positions.append((m.start(), CIRCLED_MAP[sym], sym))
    positions.sort(key=lambda x: x[0])
    return positions


def split_hang_texts(block_text: str, hang_positions):
    parts = []
    if not hang_positions:
        return parts
    for i, (pos, num, sym) in enumerate(hang_positions):
        start = pos
        end = (
            hang_positions[i + 1][0] if i + 1 < len(hang_positions) else len(block_text)
        )
        raw = block_text[start:end].lstrip()
        if raw.startswith(sym):
            raw = raw[len(sym) :].lstrip()
        parts.append((num, raw.rstrip()))
    return parts


def split_ho_with_preface(hang_text: str):
    matches = list(HO_LINE_RE.finditer(hang_text))
    if not matches:
        return hang_text.strip(), []
    preface_end = matches[0].start()
    preface = hang_text[:preface_end].strip()
    results = []
    for i, m in enumerate(matches):
        ho_no = int(m.group(1))
        start = m.start()
        end = matches[i + 1].start() if i + 1 < len(matches) else len(hang_text)
        piece = hang_text[start:end].strip()
        piece = re.sub(r"^\s*\d+\.\s*", "", piece)
        results.append((ho_no, piece.strip()))
    return preface, results


def hard_cut_article_text(article_text: str) -> str:
    m = CIRCLED_RE.search(article_text)
    if m:
        return article_text[: m.start()].rstrip()
    m2 = HANG_TEXT_RE.search(article_text)
    if m2:
        return article_text[: m2.start()].rstrip()
    return article_text.rstrip()


def build_nodes_for_cell(cell_text: str) -> List[Dict[str, Any]]:
    law_title, body = extract_law_title_and_body(cell_text)
    if not law_title:
        return []

    nodes: List[Dict[str, Any]] = []
    node_map: Dict[str, Dict[str, Any]] = {}
    chunks = split_by_articles(body)

    if not chunks and ("ë³„í‘œ" in law_title or "ë³„ì§€" in law_title):
        node_id_text = law_title
        special_node = {
            "id": node_id_text,
            "law_title": node_id_text,
            "level": "ê¸°íƒ€",
            "number": "ê¸°íƒ€",
            "parent_id": None,
            "Children_id": [],
            "text": node_id_text,
            "refs": [],
        }
        return [special_node]

    if not chunks:
        return []

    for main_no, sub_no, title, start, end in chunks:
        block = body[start:end].strip()
        m_head = JOSA_RE.match(block)
        header_txt = (
            m_head.group(0).strip() if m_head else block.split("\n", 1)[0].strip()
        )
        first_hang_idx = find_first_hang_start(block)

        if first_hang_idx != -1 and m_head:
            preface = block[m_head.end() : first_hang_idx].strip()
            article_text = header_txt if not preface else (header_txt + "\n" + preface)
        else:
            article_text = block

        article_text = hard_cut_article_text(article_text)
        article_id = make_article_id(law_title, main_no, sub_no)
        number_field = make_article_number_field(main_no, sub_no)
        art_node = {
            "id": article_id,
            "law_title": law_title,
            "level": "ì¡°",
            "number": number_field,
            "parent_id": None,
            "Children_id": [],
            "text": article_text,
            "refs": [],
        }
        nodes.append(art_node)
        node_map[article_id] = art_node

        if first_hang_idx == -1:
            continue

        after_header = block[first_hang_idx:]
        hang_positions = find_hang_positions(after_header)
        hang_parts = split_hang_texts(after_header, hang_positions)
        if not hang_parts:
            continue

        for hang_no, hang_txt in hang_parts:
            hang_preface, ho_list = split_ho_with_preface(hang_txt)
            hang_id = f"{article_id}({hang_no})"
            hang_node = {
                "id": hang_id,
                "law_title": law_title,
                "level": "í•­",
                "number": str(hang_no),
                "parent_id": article_id,
                "Children_id": [],
                "text": hang_preface,
                "refs": [],
            }
            nodes.append(hang_node)
            node_map[article_id]["Children_id"].append(hang_id)
            node_map[hang_id] = hang_node

            for ho_no, ho_txt in ho_list:
                ho_id = f"{hang_id}[{ho_no}]"
                ho_node = {
                    "id": ho_id,
                    "law_title": law_title,
                    "level": "í˜¸",
                    "number": str(ho_no),
                    "parent_id": hang_id,
                    "Children_id": [],
                    "text": ho_txt,
                    "refs": [],
                }
                nodes.append(ho_node)
                node_map[hang_id]["Children_id"].append(ho_id)
                node_map[ho_id] = ho_node

    return nodes


def read_excel_first_or_named(path: str, sheet_name=None) -> tuple[pd.DataFrame, str]:
    xls = pd.ExcelFile(path)
    chosen = (
        sheet_name
        if (sheet_name and sheet_name in xls.sheet_names)
        else xls.sheet_names[0]
    )
    print(f"  [INFO] ì‹œíŠ¸ ì„ íƒ: {chosen}")
    df = pd.read_excel(xls, sheet_name=chosen, dtype=str)
    return df, chosen


# ====================================
# í•µì‹¬ ë¡œì§ í•¨ìˆ˜
# ====================================
def process_single_file(file_base: str, sheet_name: Optional[str] = None):
    """ë‹¨ì¼ íŒŒì¼ì„ ì½ì–´ JSON ì»¬ëŸ¼ì„ ì¶”ê°€í•˜ê³  ì €ì¥í•˜ëŠ” í•¨ìˆ˜"""
    in_xlsx = f"{file_base}_labeled.xlsx"
    out_xlsx = f"{file_base}_Ref_labeled_with_json.xlsx"

    # ì…ë ¥ íŒŒì¼ í™•ì¸
    if not os.path.exists(in_xlsx):
        print(f"[SKIP] ì…ë ¥ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {in_xlsx}\n")
        return

    print(f"â–¶ï¸  '{os.path.basename(file_base)}' íŒŒì¼ ì²˜ë¦¬ ì‹œì‘...")

    df, sheet = read_excel_first_or_named(in_xlsx, sheet_name)

    col_src = "ë§í¬í…ìŠ¤íŠ¸ í´ë¦­ì‹œ ë°ì´í„°"
    if col_src not in df.columns:
        raise ValueError(f"ì…ë ¥ ì—‘ì…€ì— '{col_src}' ì»¬ëŸ¼ì´ ì—†ìŠµë‹ˆë‹¤.")

    json_col = []
    for txt in df[col_src].fillna(""):
        nodes = build_nodes_for_cell(txt)
        json_str = json.dumps(nodes, ensure_ascii=False, separators=(",", ":"))
        json_col.append(json_str)

    df["ë§í¬ë°ì´í„°_JSON"] = json_col

    with pd.ExcelWriter(out_xlsx, engine="openpyxl") as writer:
        df.to_excel(writer, index=False, sheet_name=sheet)

    print(f"âœ… ì €ì¥ ì™„ë£Œ â†’ {out_xlsx}")
    print(f"  ì´ í–‰ìˆ˜: {len(df)}\n")


# ====================================
# ì‹¤í–‰
# ====================================
if __name__ == "__main__":
    print("===== JSON ë³€í™˜ ì‘ì—… ì‹œì‘ =====")
    for file_base_path in FILES_TO_PROCESS:
        try:
            process_single_file(file_base_path)
        except Exception as e:
            file_name = os.path.basename(file_base_path)
            print(f"ğŸš¨ '{file_name}' ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ!")
            print(f"  ì˜¤ë¥˜ ë‚´ìš©: {e}\n")
    print("===== ëª¨ë“  ì‘ì—… ì™„ë£Œ =====")
